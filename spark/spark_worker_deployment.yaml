apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-worker
  template:
    metadata:
      labels:
        app: spark-worker
    spec:
      containers:
        - name: spark-worker
          image: rabii10/spark-image:v2
          imagePullPolicy: IfNotPresent

          securityContext:
            runAsUser: 0
            runAsGroup: 0

          env:
            - name: SPARK_MODE
              value: worker
            - name: SPARK_MASTER_URL
              value: spark://spark-master-service:7077
            - name: SPARK_WORKER_WEBUI_PORT
              value: "8081"
            - name: SPARK_WORKER_MEMORY
              value: "1G"
            - name: SPARK_WORKER_CORES
              value: "1"
            # important : dire à Spark quel python utiliser côté worker
            - name: PYSPARK_PYTHON
              value: "python3"

          command: ["/bin/bash","-c"]
          args:
            - |
              set -ex

              # Installer pip si besoin + libs utiles
              apt-get update
              apt-get install -y python3-pip nano iputils-ping dnsutils
              apt-get clean
              rm -rf /var/lib/apt/lists/*

              # Installer joblib côté worker (si tu en as besoin dans les tâches)
              pip3 install --no-cache-dir joblib

              # Démarrer le worker Spark
              /opt/spark/sbin/start-worker.sh spark://spark-master-service:7077

              # Garder le container up
              tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker*.out

          ports:
            - containerPort: 8081
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "1"
              memory: "1Gi"

---
apiVersion: v1
kind: Service
metadata:
  name: spark-worker-service
spec:
  type: ClusterIP
  selector:
    app: spark-worker
  ports:
    - port: 8081
      targetPort: 8081
